/*
 * MPIMatlabCommunicator.cpp
 *
 *  Created on: Apr 23, 2013
 *      Author: saul
 */

#include "MPIMatlabCommunicator.h"
/*
* Sends a vector to the specific proccess with the receiver Id
* @param receiverId, the id of the proccess that will receive the array
* @param communicator, the channel of the communication
* @param array, the array to send
*/
void sendVector(int receiverId,  vector array ){
	MPI_Request request, request2;
	int size = array.x;
	//sends the size of the array first
	MPI_Send( &size, 1, MPI_INT, receiverId, VECTOR_TAG, communicator);
	//Sends the array
	MPI_Send( array.v, size, MPI_DOUBLE, receiverId, VECTOR_TAG, communicator);
}
/*
* Receives vector from the specific proccess with the sender Id
* @param senderId, the id of the proccess that sent  the array
* @param communicator, the channel of the communication
* @param array, the pointer where the received array will be stored
*/
void receiveVector(int senderId, vector* ptrArray ){
	int size = 0;
	MPI_Status status;
	//Receives the size of the array first
	MPI_Recv(&size, 1, MPI_INT, senderId, VECTOR_TAG, communicator, &status);
	//Receives the array
	*ptrArray = zerov( size );
	MPI_Recv(ptrArray -> v, size, MPI_DOUBLE, senderId, VECTOR_TAG, communicator, &status);
}

/*
* Executes the MPI function reduce
* @param proccessId, the id of the proccess that executes the operation
* @param communicator, the channel of the communication
* @param array, the array that will be send
* @param operation, the operation to be executed with the array as the input and result as the output
* @param result, the result of the operation
*
*/
void reduceVector(int proccessId, vector array, MPI_Op operation, vector* result){
	MPI_Reduce(array.v, result->v, array.x, MPI_DOUBLE, operation, proccessId, communicator);
}

/*
* Executes the MPI function reduce
* @param proccessId, the id of the proccess that executes the operation
* @param communicator, the channel of the communication
* @param M, the matrix that will be send
* @param operation, the operation to be executed with the array as the input and result as the output
* @param resultM, the result of the operation
*
*/
void reduceMatrix(int proccessId, matrix M, MPI_Op operation, matrix* resultM){
	vector extendedM = zerov( M.x * M.y );
	vector extendedR = zerov( M.x * M.y );
	int i, j;
	int k = 0;
	//Translates from Matrix to vector
	for( i = 0; i < M.x; ++i ){
		for( j = 0; j < M.y; ++j){
			extendedM.v[ k++ ] = M.m[ i ][ j ];
		}
	}
	MPI_Reduce(extendedM.v, extendedR.v, extendedM.x, MPI_DOUBLE, operation, proccessId, communicator);
	//Translates from vector to matrix
	k = 0;
	for( i = 0; i < M.x; ++i ){
		for( j = 0; j < M.y; ++j){
			resultM -> m[ i ][ j ] = extendedR.v[ k++ ];
		}
	}
}


/*
* Broadcasts a vector to the children
* @param numberOfChildren, children number
* @param communicator, the channel of the communication
* @param array, the array that will be send
*/
vector broadcastVector(int procId, vector array){
	//we cannot send one matrix at once, since the rows are not contiguos
	int i;

	if(DEBUG == 1)printf("BroadcastVector: init: %d\n", procId);
	int x = array.x;
	//sends the row number
	if(DEBUG == 1)printf("Broadcast: sending/rec dimension1: %d\n", procId);
	MPI_Bcast( &x, 1, MPI_INT, 0,  communicator);
	//sends the array
	if(procId != 0){
		array = zerov( x );
	}
	if(DEBUG == 1)printf("Broadcast: sending/rec array: %d\n", procId);
	MPI_Bcast( array.v, x, MPI_DOUBLE, 0, communicator);
	if(DEBUG == 1)printf("Broadcast: array received/sent: %d\n", procId);
	return array;
}


/*
* Broadcasts a matrix to the children
* @param numberOfChildren, children number
* @param communicator, the channel of the communication
* @param M, the matrix that will be send
*/
matrix broadcastMatrix(int procId, matrix M){
	//we cannot send one matrix at once, since the rows are not contiguos
	matrix receivedM;
	if(DEBUG == 1)printf("Broadcast: init: %d\n", procId);
	int i, j, k;
	vector extendedM;
	if(procId == 0){
		//in case that it is the sender
		extendedM = zerov( M.x * M.y );
		k = 0;
		for( i = 0; i < M.x; ++i ){
			for( j = 0; j < M.y; ++j){
				extendedM.v[ k++ ] = M.m[ i ][ j ];
			}
		}
	}
	else{
		M = zerom(2,2);
	}
	int x = M.x;
	int y = M.y;
	//sends the row number
	if(DEBUG == 1)printf("Broadcast: sending/rec dimension1: %d\n", procId);
	MPI_Bcast( &x, 1, MPI_INT, 0,  communicator);
	if(DEBUG == 1)printf("Broadcast: sending/rec dimension2: %d\n", procId);
	//sends the column number
	MPI_Bcast( &y, 1, MPI_INT, 0, communicator);
	//sends the array

	if(procId != 0){
		extendedM = zerov( x * y );
	}

	if(DEBUG == 1)printf("Broadcast: sending/rec array: %d\n", procId);
	MPI_Bcast( extendedM.v, extendedM.x, MPI_DOUBLE, 0, communicator);
	if(DEBUG == 1)printf("Broadcast: array received/sent: %d\n", procId);
	k = 0;
	receivedM = zerom( x, y );
	for( i = 0; i < x; ++i ){
		for( j = 0; j < y; ++j){
			receivedM.m[ i ][ j ] = extendedM.v[ k++ ];
		}
	}
	return receivedM;
}

/*
* Sends a Matrix to the specific proccess with the receiver Id
* @param receiverId, the id of the proccess that will receive the array
* @param communicator, the channel of the communication
* @param matrix, the matrix to send
*/
void sendMatrix(int receiverId, matrix M ){
	//we cannot send one matrix at once, since the rows are not contiguos
	MPI_Request request, request2;
	vector extendedM = zerov( M.x * M.y );
	int i, j;
	int k = 0;
	for( i = 0; i < M.x; ++i ){
		for( j = 0; j < M.y; ++j){
			extendedM.v[ k++ ] = M.m[ i ][ j ];
		}
	}
	//sends the row number
	MPI_Send( &M.x, 1, MPI_INT, receiverId, MATRIX_TAG, communicator);
	//sends the column number
	MPI_Send( &M.y, 1, MPI_INT, receiverId, MATRIX_TAG, communicator);
	//sends the array
	sendVector( receiverId, extendedM );
}

/*
* Receives the matrix from the specific proccess with the sender Id
* @param senderId, the id of the proccess that sent  the array
* @param communicator, the channel of the communication
* @param matrix, the pointer where the received matrix will be stored
*/
void receiveMatrix(int senderId, matrix* M ){
	int x, y;
	int k = 0;
	int i, j;
	MPI_Status status;
	vector array = zerov(2);
	//procId a vector that contains the rows of the original matrix contiguos
	//procId the row number
	MPI_Recv(&x, 1, MPI_INT, senderId, MATRIX_TAG, communicator, &status);
	//procId the column number
	MPI_Recv(&y, 1, MPI_INT, senderId, MATRIX_TAG, communicator, &status);
	//procId the array
	receiveVector( senderId, &array );
	*M = zerom( x, y );
	for( i = 0; i < x; ++i ){
		for( j = 0; j < y; ++j ){
			M -> m[ i ][ j ] = array.v[ k++ ];
		}
	}
}

/*
* Broadcasts a double to the children
* @param numberOfChildren, children number
* @param communicator, the channel of the communication
* @param number, the number that will be send
*/
double broadcastDouble( double number ){
	MPI_Bcast( &number, 1, MPI_DOUBLE, 0,  communicator);
	return number;
}

/*
* Sends a Double to the specific proccess with the receiver Id
* @param receiverId, the id of the proccess that will receive the array
* @param communicator, the channel of the communication
* @param number, the number to send
*/
void sendDouble(int receiverId, double number){
	MPI_Send( &number, 1, MPI_DOUBLE, receiverId, DOUBLE_TAG, communicator);
}

/*
* Receives the double from the specific proccess with the sender Id
* @param senderId, the id of the proccess that sent  the array
* @param communicator, the channel of the communication
* @param double, the pointer where the received double will be stored
*/
void receiveDouble(int senderId, double* number ){
	MPI_Status status;
	//Receives the size of the array first
	MPI_Recv( number, 1, MPI_DOUBLE, senderId, DOUBLE_TAG, communicator, &status);
}

/*
*Creates the received number of processes, with the children in one group and the parent in other one
*@param np, number of processes to spawn
*@param argv, arguments for the executable
*/
void createProcesses( int np,  char *argv[] ){
	MPI_Comm universeComm, interComm, parentComm;
	MPI_Comm_get_parent( &parentComm );
	communicator = MPI_COMM_WORLD;
	if(np > 0){
		if(parentComm == MPI_COMM_NULL){
		  	char *cmds[np];
		    	MPI_Info infos[np];
			int errcodes[ np ];
			char** argvs[ np ];
			int maxProcs[ np ];
			int i = 0;
			for( i = 0; i < np; ++i ){
				cmds[ i ] = EXE_NAME;
				infos[ i ] = MPI_INFO_NULL;
				argvs[ i ] = argv;
				maxProcs[ i ] = 1;
			}
			  MPI_Comm_spawn_multiple( np, cmds, argvs, maxProcs, infos, 0, MPI_COMM_WORLD, &interComm, errcodes );
			//Merges the intercoms
			MPI_Intercomm_merge(  interComm, 1, &universeComm );
		}
		else{
			//Merges the intercoms
			MPI_Intercomm_merge(  parentComm, 1, &universeComm );
		}
		//stores the communicator
		communicator = universeComm;
	}
}

/*
*Creates the received number of processes, with the children in one group and the parent in other one, designed for the ECCI's cluster
*@param np, number of processes to spawn
*@param argv, arguments for the executable
*/
void createProcessesCluster( int np,  char *argv[] ){
	MPI_Comm universeComm, interComm, parentComm;
	char* nodes[4] = { "arenal", "compute-0-0", "compute-0-1", "compute-0-2" };
	MPI_Comm_get_parent( &parentComm );
	//communicator = MPI_COMM_WORLD;
	if(np > 0){
		if(parentComm == MPI_COMM_NULL){
		  	char *cmds[np];
		    	MPI_Info infos[np];
			int errcodes[ np ];
			char** argvs[ np ];
			int maxProcs[ np ];
			int i = 0;
			for( i = 0; i < np; ++i ){
				cmds[ i ] = EXE_NAME;
				MPI_Info_create( &infos[ i ] );
				MPI_Info_set( infos[ i ], "host", nodes[ i ] );
				argvs[ i ] = argv;
				maxProcs[ i ] = 1;
			}
			MPI_Comm_spawn_multiple( np, cmds, argvs, maxProcs, infos, 0, MPI_COMM_WORLD, &interComm, errcodes );
			//Merges the intercoms
			MPI_Intercomm_merge(  interComm, 1, &universeComm );
		}
		else{
			//Merges the intercoms
			MPI_Intercomm_merge(  parentComm, 1, &universeComm );
		}
		//stores the communicator
		communicator = universeComm;
	}
}

/*
*Returns wether the current process is the parent
*@return 1 if so, 0 otherwise
*/
int isParent(){
	MPI_Comm parentComm;
	MPI_Comm_get_parent( &parentComm );
	return (parentComm == MPI_COMM_NULL);
}

/*
*Returns the current process id
*@return process id
*/
int getProcessId(){
	int myid = 0;
	//if the communicator is null, the operation is invalid
	assert (communicator != MPI_COMM_NULL);
	MPI_Comm_rank( communicator, &myid );
	return myid;
}


/*
*initializes MPI
*@param argc
*@param argv
*/
void initMPI(int argc, char *argv[]){
	MPI_Init( &argc, &argv );
}



/*
*Generates the processes workload
*@param paralelismlevel, 1 maximum, 0 minimum
*@param numberOfWs, number of window sizes or total work units
*/
void generateWindowSizesRanges(double paralelismLevel, int numberOfWs, matrix* ranges ){
	int numberOfProcs, windowsPerProc, currentProc, i, firstWindow;
	numberOfProcs = (int) (paralelismLevel * numberOfWs);
	if(numberOfProcs == 0) numberOfProcs = 1;
	*ranges = zerom( numberOfProcs, 2 );
	printf("\nnumberOfProcs %d\n", numberOfProcs);
	//round up to ensure that every windows size is taken by all the processes
	if( (numberOfWs % numberOfProcs ) != 0 ){
		windowsPerProc = (int)(numberOfWs / numberOfProcs) + 1;
	}
	else{
		windowsPerProc = (int)(numberOfWs / numberOfProcs);
	}
	printf("\nwindowsPerProc %d\n", windowsPerProc);
	currentProc = 0;
	firstWindow = 0;
	for( i = 0; i < numberOfWs; ++i ){
		if( i % windowsPerProc == 0 && i != 0 ){
			ranges->m[ currentProc ][ 0 ] = firstWindow;
			//last window in range
			ranges->m[ currentProc++ ][ 1 ] = i;
			firstWindow = i;
		}
	}
	//the last range for the last proccess
	ranges->m[ currentProc ][ 0 ] = firstWindow;
	ranges->m[ currentProc++ ][ 1 ] = numberOfWs;
	numberOfProcs = currentProc;
	ranges->x = currentProc;
	printf("\ncurrentProc %d\n", currentProc);
}

/*
*Finalizes MPI
*/
void finalizeMPI(){
	MPI_Finalize();
}


